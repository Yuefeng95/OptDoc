- [1. 概述 ———— 为何进行多核优化](#1-概述--为何进行多核优化)
- [2. 多核框架的应用模型](#2-多核框架的应用模型)
	- [2.1. 主\从模型 (Master/Slave)](#21-主从模型-masterslave)
	- [2.2. 数据流模型 (Data Flow)](#22-数据流模型-data-flow)
	- [2.3. OpenMP 模型](#23-openmp-模型)
	- [2.4. 如何选择 主从模型 or 数据流模型](#24-如何选择-主从模型-or-数据流模型)
- [3. 优化多核程序的步骤](#3-优化多核程序的步骤)
- [4. 内核间互动](#4-内核间互动)
	- [4.1. Data Movement](#41-data-movement)
		- [4.1.1. Data Movement Engine](#411-data-movement-engine)
	- [4.2. Notification & Synchronization](#42-notification--synchronization)
		- [4.2.1. Direct Signaling](#421-direct-signaling)
		- [4.2.2. Indirect Signaling](#422-indirect-signaling)
		- [4.2.3. Atomic Arbitration](#423-atomic-arbitration)
		- [4.2.4. Multicore Navigator Notification Methods](#424-multicore-navigator-notification-methods)
- [5. Shared Resource Management](#5-shared-resource-management)
- [6. 内存管理](#6-内存管理)
	- [6.1. CPU 的内存操作硬件](#61-cpu-的内存操作硬件)
	- [6.2. Cache and Prefetch Considerations](#62-cache-and-prefetch-considerations)
	- [6.3. 数据分布方案](#63-数据分布方案)

> ref: [Multicore Programming Guide.pdf](https://github.com/Yuefeng95/CloudDoc/blob/main/%E6%94%B6%E8%97%8F%E6%96%87%E6%A1%A3/TDA4VM/c66/Multicore%20Programming%20Guide.pdf)

# 1. 概述 ———— 为何进行多核优化

在过去的 50 年里，摩尔定律准确地预测，集成电路上的晶体管数量每两年就会翻一番。为了将这些晶体管转化为同等水平的系统性能，芯片设计人员做了以下努力：

- 时钟频率（需要更深的指令流水线）
- 提高指令级并行性（需要并发线程和分支预测）
- 提高内存性能（需要更大的缓存）
- 增加功耗（需要主动电源管理）

这四个领域中的每一个都在碰壁，阻碍了进一步的增⻓：

- 增加的处理频率正在减慢，因为随着半导体器件的缩小，时钟速率的改进减少以及线缩放不良
- 指令级并行性受到应用程序内在缺乏并行性的限制
- 内存性能受到处理器和内存速度之间不断扩大的差距的限制
- 功耗与时钟频率成正比；因此，在某些时候，需要特殊的方法来冷却设备

在此背景下，通过多芯片实现系统性能提升成为新的思路。

# 2. 多核框架的应用模型

多核框架适用的应用模型可概括为三种，主\从架构、数据模型、OpenMP模型。

## 2.1. 主\从模型 (Master/Slave)

主\从模型代表分布式执行的集中处理。

| 适用于主\从模型的程序特性 |                                                                |
| ------------------------- | -------------------------------------------------------------- |
| 线程方面                  | 应用程序由许多独立的线程组成，每个线程都能在单个内核资源上运行 |
| 控制方面                  | 包含大量控制代码                                               |
| 访存方面                  | 包含多个明显分割的随机顺序内存访问，每次内存访问计算量较少     |

| Master 核心的工作                      |
| -------------------------------------- |
| 运行 Linux 等高级操作系统              |
| 执行控制，调度线程，分配各个核心的业务 |
| 分配各个核心需要的数据                 |

| Slave 核心的工作                                                                                |
| ----------------------------------------------------------------------------------------------- |
| 内核之间 IPC 实现。每个核心至少有一个任务，其工作是接收信息。当内核 idle 时会挂起指导接收消息。 |

**需要重点解决的问题**

需要解决实时负载均衡的问题。因为程序 线程的激活 或 线程中任务来的吞吐量 随时都会变动。Master需要维护可用资源的列表，并能实现内核之间的工作平衡，实现最佳的并行度。

<img src="https://raw.githubusercontent.com/Yuefeng95/Images/main/202203041916295.png" height="400px" /> 

## 2.2. 数据流模型 (Data Flow)

数据流模型代表分布式控制和执行。

| 数据流模式的特征                                           |
| ---------------------------------------------------------- |
| 初始内核为 传感器 或 FPGA 处理                             |
| 调度是在数据可用时触发                                     |
| 可能运行在 RTOS，最小化延迟是关键指标                      |
| 数据访问模式非常有规律，数据数组的每个元素都经过相同的处理 |

**需要重点解决的问题**

需要解决核心之间低延迟切换的问题。切换核心主要开销在数据流切换。主要通过维持 任务pipline 周期性运转 和 内核之间良好带宽。

| 核心间高效内存流动方案 |          |
| ---------------------- | -------- |
| DMA传输                | 共享内存 |

<img src="https://raw.githubusercontent.com/Yuefeng95/Images/main/202203041951963.png" height="192px" /> 

## 2.3. OpenMP 模型

OpenMP 通过并行宏和API实现代码并行化改造。依赖 OpenMP 提供的并行化改造能力，实现 任务的并发 和 自动同步。

| 并行化命令           | Runtime并行化效果        |
| -------------------- | ------------------------ |
| #pragma omp parallel | 执行预设的并行数量的线程 |
| #pragma omp for      | 对for循环执行并行处理    |
| #pragma omp sections | 分配跨内核任务           |

**分配内核并行的代码示例**

<img src="https://raw.githubusercontent.com/Yuefeng95/Images/main/202203071043460.png" height="150px" /> 

## 2.4. 如何选择 主从模型 or 数据流模型

| item                 | 主从模型         | 数据流模型                      | 结论                                                                   |
| -------------------- | ---------------- | ------------------------------- | ---------------------------------------------------------------------- |
| 算子高性能计算特效型 | 访存密集型       | 计算密集型                      | 非访存瓶颈算子 在 数据流模型下，能使每个芯片做最合适的事，充分发挥算力 |
| 产品运行特性         | 帧率处于动态稳定 | 帧率趋于平稳                    |                                                                        |
| 如何提升性能         | 负载均衡         | 降低内核间 通信、内存搬运的延迟 |                                                                        |


# 3. 优化多核程序的步骤

| 步骤 | 动作          |                                                                                                                                    |
| ---- | ------------- | ---------------------------------------------------------------------------------------------------------------------------------- |
| 1    | Partitioning  | 定义并行任务大小                                                                                                                   |
| 2    | Communication | 定义并行任务之间传输的数据                                                                                                         |
| 3    | Combining     | 保证任务划分。将低计算高耦合模块组合；将高通信高计算模块分解为较小的模块                                                           |
| 4    | Mapping       | 1. 选择模型。 Master/Slave 或 Data Flow                                                                                            |
|      |               | 2. 平衡负载。保存第一次迭代时初始化的 MIPS、L2 Memory、Communiction Bandwidth 资源。在每个核心的负载进行衡量，对不平衡部分进行重构 |
|      |               | 3. 降低延迟，减少同步。减少多核共用资源(DMA等)，使用硬件信号量确保互斥。                                                           |
|      |               | 4. 利用芯片内存进行 内存缓冲 或 复制，补偿处理期间通信延迟                                                                         |
|      |               | 5. 分割吞吐量超过单核性能的处理到多核，提高并行性。                                                                                |
|      |               | 6. 将需要协处理器功能划分到单个内核，降低通信成本                                                                                  |
|      |               | 7. 对过个周期的任务分配和并行效率进行测量和优化                                                                                    |
| 5    | OpenMP        | 考虑通过 OpenMP Section 实现多核并行                                                                                               |

# 4. 内核间互动

内核通信主要包含两个动作：Data Movement 、Notification (including synchronization) 。   
内核见通信的实现方式包含 信号通信、短消息(内存)通信、数据移动。其中信号是指硬件设计的中断事件、传递共享资源所有权的atomic access arbitratio等信息。短消息和数据都是内存。

## 4.1. Data Movement

- Shared Message Buffer 通过 SYS/BIOS 消息队列传递短消息。如消息队列。
- Dedicated Memories 通过消息和固定内存传递
- Transitioned Memory Buffer 通过内存指针传递
- OpenMP 使用宏自动声明 私有 或 共享 内容
- Multicore Navigator Data Movement // TODO

<img src="https://raw.githubusercontent.com/Yuefeng95/Images/main/202203071201968.png" height="150px" /> 	

### 4.1.1. Data Movement Engine

C66 的相关模块是 EDMA (enhanced DMA) 和 属于 Multicore-Navigator 的 PKTDMA (Packet DMA) 。PKTDMA 用于内核与 high-bit-rate 设备交互数据。

## 4.2. Notification & Synchronization

| 同步和通知的实现方式                          |
| --------------------------------------------- |
| Multicore-Navigator 或 CPU 控制               |
| 内核间 (直接或间接) 通信 或 Atomic Arbitratio |
| OpemMP 自动同步                               |

### 4.2.1. Direct Signaling

The processing steps are:
1. CPU A writes to CPU B’s inter-processor communication (IPC) control register
2. IPC event generated to interrupt controller
3. Interrupt controller notifies CPU B (or polls)
4. CPU B queries IPC
5. CPU B clears IPC flag(s)
6. CPU B performs appropriate action

### 4.2.2. Indirect Signaling

e.g. EDMA    
The processing steps are:
1. CPU A configures and triggers transfer using EDMA
2. EDMA completion event generated to interrupt controller
3. Interrupt controller notifies CPU B (or polls)


### 4.2.3. Atomic Arbitration

通过核心访问共享硬件的 Atomic Arbitration 状态实现通信。

### 4.2.4. Multicore Navigator Notification Methods

 // TODO

# 5. Shared Resource Management

包含 内核间 Drect signaling 和 Atomic arbitration；内核内 Global flag 和 OS信号量。通过使用共享资源管理实现内核间通信比全局标志为更高效。

| Shared Resource Management 分类 |
| ------------------------------- |
| Global Flags                    |
| OS Semaphores                   |
| Hardware Semaphores             |
| Direct Signaling                |

# 6. 内存管理

## 6.1. CPU 的内存操作硬件

<img src="https://raw.githubusercontent.com/Yuefeng95/Images/main/202203071529321.png" height="300px" /> 

Each of the cores is a master to both the configuration (access to peripheral control    
registers) and DMA (internal and external data memories) switch fabrics. In addition,    
each core has a slave interface to the DMA switch fabric allowing access to its L1 and    
L2 SRAM. All cores have equal access to all slave endpoints with priority assigned per    
master by user software for arbitration between all accesses at each endpoint.   

图中可见，Shared Memory 在 MSMC 上。

## 6.2. Cache and Prefetch Considerations

L1/L2 cache 与 Shared Memory 没有一致性。

<img src="https://raw.githubusercontent.com/Yuefeng95/Images/main/202203071623354.png" height="400px" /> 

## 6.3. 数据分布方案

| 情况           | 方案                                                             |
| -------------- | ---------------------------------------------------------------- |
| 最优           | 全部重要数据在 L2 SRAM 上                                        |
| L2 SRAM 不足时 | 通过 DMA + ping-pong buffer 传递 外部内存(如DDR) 与 L2 SRAM 数据 |

